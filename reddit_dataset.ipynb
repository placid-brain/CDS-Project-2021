{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentleonardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentleonardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vincentleonardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "\n",
    "#import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences # There's a PyTorch implementation but for Tensors.\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('~/Downloads/reddit_mbti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mbti_extract(text):\n",
    "    if re.search('(infp|INFP)', text):\n",
    "        return 'INFP'\n",
    "    elif re.search('(enfp|ENFP)', text):\n",
    "        return 'ENFP'\n",
    "    elif re.search('(istj|ISTJ)', text):\n",
    "        return 'ISTJ'\n",
    "    elif re.search('(istp|ISTP)', text):\n",
    "        return 'ISTP'\n",
    "    elif re.search('(isfj|ISFJ)', text):\n",
    "        return 'ISFJ'\n",
    "    elif re.search('(isfp|ISFP)', text):\n",
    "        return 'ISFP'\n",
    "    elif re.search('(infj|INFJ)', text):\n",
    "        return 'INFJ'\n",
    "    elif re.search('(intj|INTJ)', text):\n",
    "        return 'INTJ'\n",
    "    elif re.search('(intp|INTP)', text):\n",
    "        return 'INTP'\n",
    "    elif re.search('(estp|ESTP)', text):\n",
    "        return 'ESTP'\n",
    "    elif re.search('(estj|ESTJ)', text):\n",
    "        return 'ESTJ'\n",
    "    elif re.search('(esfp|ESFP)', text):\n",
    "        return 'ESFP'\n",
    "    elif re.search('(esfj|ESFJ)', text):\n",
    "        return 'ESFJ'\n",
    "    elif re.search('(enfj|ENFJ)', text):\n",
    "        return 'ENFJ'\n",
    "    elif re.search('(entp|ENTP)', text):\n",
    "        return 'ENTP'\n",
    "    elif re.search('(entj|ENTJ)', text):\n",
    "        return 'ENTJ'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['MBTI'] = df.flair_text.apply(mbti_extract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.MBTI.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.body.str.len().plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites for this function\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "def preprocessing(text):\n",
    "    \"\"\"Preprocessing each row of text\n",
    "    \n",
    "    List of things done:\n",
    "    - Lower casing\n",
    "    - Remove URL links\n",
    "    - Remove mention/hashtags (@anonymous, #anonymous)\n",
    "    - Remove accented characters (changing to ASCII from UTF-8)\n",
    "    - Remove punctuations, numbers, and irrelevant characters\n",
    "    - Remove excess whitespace\n",
    "    - Remove stop words in English\n",
    "    - Normalise and lemmatise\n",
    "    - Remove single characters\n",
    "    \n",
    "    Possible improvements:\n",
    "    - Separating words from numbers (if it exists)\n",
    "    - Convert emoji and chat lingo with proper form\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        text (str): String of text that needs to be preprocessed\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(\"@\\S+\", \" \", text)\n",
    "    text = re.sub(\"#\\S+\", \" \", text)\n",
    "    # text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = [contractions.fix(word) for word in text.split()]\n",
    "    text = ' '.join([word for word in text])\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text) # re.sub(r'[^a-zA-Z0-9]', ' ', text) if we want numeric characters\n",
    "    text = re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "    # text = word_tokenize(text)\n",
    "    # text = [token for token in text if token not in stopwords.words('english')]\n",
    "    # text = [WordNetLemmatizer().lemmatize(word, pos='v') for word in text]\n",
    "    # text = [WordNetLemmatizer().lemmatize(word, pos='a') for word in text]\n",
    "    # text = [word for word in text if len(word) > 1]\n",
    "    # text = ' '.join([word for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df.body.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'] = df.body.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.body != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 40000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 750 # max number of words in a question to use\n",
    "batch_size = 512 # how many samples to process at once\n",
    "n_epochs = 5 # how many times to iterate over all samples\n",
    "n_splits = 5 # Number of K-fold Splits\n",
    "SEED = 10\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def keras_tokenizer(train_X, test_X):\n",
    "    \"\"\"Tokenize and pad the split train and test features\n",
    "\n",
    "    Args:\n",
    "        train_X (array): Training features\n",
    "        test_X (array): Test features\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    # Pad the sentences\n",
    "    train_X = pad_sequences(train_X)\n",
    "    test_X = pad_sequences(test_X)\n",
    "    \n",
    "    return train_X, test_X, tokenizer\n",
    "\n",
    "def label_encoder(train_y, test_y):\n",
    "    le = LabelEncoder()\n",
    "    train_y = le.fit_transform(train_y.values)\n",
    "    test_y = le.transform(test_y.values)\n",
    "    \n",
    "    return train_y, test_y, le\n",
    "\n",
    "def load_fasttext(word_index):\n",
    "    \n",
    "    EMBEDDING_FILE = 'input/wiki-news-300d-1M.vec'\n",
    "    \n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \"))\n",
    "                            for o in open(EMBEDDING_FILE) \n",
    "                            if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        filter_sizes = [1,2,3,5]\n",
    "        num_filters = 36\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, num_filters, (K, embed_size)) for K in filter_sizes])\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1)  \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  \n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  \n",
    "        logit = self.fc1(x) \n",
    "        return logit\n",
    "    \n",
    "class BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        drp = 0.1\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*4 , 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.out = nn.Linear(64, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #rint(x.size())\n",
    "        h_embedding = self.embedding(x)\n",
    "        #_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_exploded, X_test_exploded, y_train_exploded, y_test_exploded = train_test_split(df.body, \n",
    "                                                                                        df.MBTI, random_state=0,\n",
    "                                                                                        test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_exploded, X_test_exploded, tokenizer = keras_tokenizer(X_train_exploded, X_test_exploded)\n",
    "y_train_exploded, y_test_exploded, le = label_encoder(y_train_exploded, y_test_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/env_datasci/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = load_fasttext(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "model = CNN_Text()\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "model#.cuda()\n",
    "\n",
    "def training(trg_train_X, trg_train_y, trg_test_X, trg_test_y):\n",
    "\n",
    "    # Load train and test in CUDA Memory\n",
    "    x_train = torch.tensor(trg_train_X, dtype=torch.long)#.cuda()\n",
    "    y_train = torch.tensor(trg_train_y, dtype=torch.long)#.cuda()\n",
    "    x_cv = torch.tensor(trg_test_X, dtype=torch.long)#.cuda()\n",
    "    y_cv = torch.tensor(trg_test_y, dtype=torch.long)#.cuda()\n",
    "\n",
    "    # Create Torch datasets\n",
    "    train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "    # Create Data Loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        # Set model to train configuration\n",
    "        model.train()\n",
    "        avg_loss = 0.  \n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            # Predict/Forward Pass\n",
    "            y_pred = model(x_batch)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        # Set model to validation configuration -Doesn't get trained here\n",
    "        model.eval()        \n",
    "        avg_val_loss = 0.\n",
    "        val_preds = np.zeros((len(x_cv),len(le.classes_)))\n",
    "        \n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            # keep/store predictions\n",
    "            val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()\n",
    "        \n",
    "        # Check Accuracy\n",
    "        val_accuracy = sum(val_preds.argmax(axis=1)==trg_test_y)/len(trg_test_y)\n",
    "        train_loss.append(avg_loss)\n",
    "        valid_loss.append(avg_val_loss)\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
    "                    epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x1/zp6kq1kj15v_md17wpr_q_n40000gn/T/ipykernel_18105/3766246014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_exploded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_exploded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_exploded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_exploded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/x1/zp6kq1kj15v_md17wpr_q_n40000gn/T/ipykernel_18105/542824773.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(trg_train_X, trg_train_y, trg_test_X, trg_test_y)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Predict/Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_datasci/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x1/zp6kq1kj15v_md17wpr_q_n40000gn/T/ipykernel_18105/1036571795.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x1/zp6kq1kj15v_md17wpr_q_n40000gn/T/ipykernel_18105/1036571795.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_datasci/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_datasci/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/env_datasci/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training(X_train_exploded, y_train_exploded, X_test_exploded, y_test_exploded)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d048e5a96ab3efc3bbb89cf19027d89d3dea82b582168492092a5bbe35e6db15"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env_datasci': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
