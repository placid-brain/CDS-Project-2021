{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {
     "execution_count": 1,
     "id": "8c176b4a-7228-4f06-86ea-ef157db54721",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/std.py:725: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import re\n",
    "import torch\n",
    "\n",
    "#import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences # There's a PyTorch implementation but for Tensors.\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gradient": {
     "execution_count": 2,
     "id": "fb3dfd03-c56b-4929-8c48-b1679b050cec",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 40000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 750 # max number of words in a question to use\n",
    "batch_size = 128 # how many samples to process at once\n",
    "n_epochs = 5 # how many times to iterate over all samples\n",
    "n_splits = 5 # Number of K-fold Splits\n",
    "SEED = 10\n",
    "debug = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM4cuug2Vgoh"
   },
   "source": [
    "### load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "gradient": {
     "execution_count": 3,
     "id": "b0237b09-1290-4917-9ef4-764f8d101ca8",
     "kernelId": ""
    },
    "id": "whBeBw94Vgoi",
    "outputId": "e7387658-ffa8-4484-b57f-0f474bea9ef9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts_length</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>304</td>\n",
       "      <td>enfp intj moments sportscenter top ten play pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>554</td>\n",
       "      <td>find lack post alarm sex bore position often e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>427</td>\n",
       "      <td>good one course say know bless curse absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>521</td>\n",
       "      <td>dear intp enjoy conversation day esoteric gabb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>477</td>\n",
       "      <td>fire another silly misconception approach logi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  posts_length                                         lemmatized\n",
       "0  INFJ           304  enfp intj moments sportscenter top ten play pr...\n",
       "1  ENTP           554  find lack post alarm sex bore position often e...\n",
       "2  INTP           427  good one course say know bless curse absolutel...\n",
       "3  INTJ           521  dear intp enjoy conversation day esoteric gabb...\n",
       "4  ENTJ           477  fire another silly misconception approach logi..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('mbti_rm_stop_lemmatized.csv', index_col=None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTERk6DAVgok"
   },
   "source": [
    "### encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "execution_count": 4,
     "id": "eb3ceece-b73a-420f-87f4-0574d9bbc342",
     "kernelId": ""
    },
    "id": "KOEKiuHHVgok"
   },
   "outputs": [],
   "source": [
    "int2mbti={0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISTJ',15:'ISTP'}\n",
    "mbti2int={'ENFJ':0,'ENFP':1,'ENTJ':2,'ENTP':3,'ESFJ':4,'ESFP':5,'ESTJ':6,'ESTP':7,'INFJ':8,'INFP':9,'INTJ':10,'INTP':11,'ISFJ':12,'ISFP':13,'ISTJ':14,'ISTP':15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 5,
     "id": "94949e1e-0123-4d22-bebe-6f2e3bbb75ff",
     "kernelId": ""
    },
    "id": "jk4TsUBYVgok",
    "outputId": "df80d3ec-41f9-47e7-fb84-d86a3d6f3974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 10, 1, 15, 11, 13, 9, 14, 3, 9, 8, 9, 10, 9, 3, 11, 9, 8, 3, 8, 8, 9, 11, 9, 9, 8, 15, 12, 9, 14, 8, 8, 10, 4, 1, 9, 3, 10, 11, 11, 3, 10, 3, 10, 8, 13, 1, 11, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "labels = df.type.tolist()\n",
    "labels = [mbti2int.get(label) for label in labels]\n",
    "print(labels[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqwhztRQVgol"
   },
   "source": [
    "### clean posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 6,
     "id": "97630665-79e7-4f2f-af85-5b8c5c4cb3e5",
     "kernelId": ""
    },
    "id": "euKkHNpGXFWw",
    "outputId": "ed516ae6-6f5b-4ab9-addd-3630764abc83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /opt/conda/lib/python3.8/site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 7,
     "id": "7ee68475-e09d-4ee7-9b8c-deec2802e144",
     "kernelId": ""
    },
    "id": "Vjp0Z2JqVgol",
    "outputId": "4a812a7c-3038-429d-8735-25a713d067c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from autocorrect import Speller \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class data_preprocessing():\n",
    "    \n",
    "    def remove_links(text):\n",
    "        remove_https = re.sub(r'http\\S+', '', text)\n",
    "        remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "        return remove_com\n",
    "    \n",
    "    def remove_digits(text):\n",
    "        return re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    def remove_symbols(text):\n",
    "        REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "        BAD_SYMBOLS_RE = re.compile('(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
    "        t = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "        t = BAD_SYMBOLS_RE.sub(' ', t)\n",
    "        return t\n",
    "    \n",
    "    def deduce_repeated_characters(text):\n",
    "        Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "        Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "        Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "        Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "        Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "        return Final_Formatted\n",
    "    \n",
    "    def remove_special_characters(text):\n",
    "        return re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text)\n",
    "    \n",
    "    def spelling_correction(text):\n",
    "        spell = Speller(lang='en')\n",
    "        Corrected_text = spell(text)\n",
    "        return Corrected_text\n",
    "    \n",
    "    def lemmatization(text):\n",
    "        w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gradient": {
     "execution_count": 8,
     "id": "be7764de-f5ad-43e6-aa1a-91b613fed856",
     "kernelId": ""
    },
    "id": "5vRxZq8gVgom"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = t.replace(\"|||\",\" \")   \n",
    "    t = data_preprocessing.remove_links(t)\n",
    "    t = data_preprocessing.remove_digits(t)\n",
    "    t = data_preprocessing.remove_symbols(t)\n",
    "    t = data_preprocessing.deduce_repeated_characters(t)\n",
    "    t = data_preprocessing.remove_special_characters(t)\n",
    "#         t = data_preprocessing.spelling_correction(t)\n",
    "    t = data_preprocessing.lemmatization(t)\n",
    "    t = ' '.join(word for word in t.split() if word not in STOPWORDS) \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {
     "execution_count": 9,
     "id": "286d8c59-56ee-47e1-8e79-b598c00c0057",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['lemmatized'],\n",
    "                                                    df['type'],\n",
    "                                                    stratify=df['type'], \n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {
     "execution_count": 10,
     "id": "353af3db-6bb9-4525-aa7f-1a1015d61347",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "x_train = pad_sequences(train_X)\n",
    "x_test = pad_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {
     "execution_count": 11,
     "id": "0e6dd728-2b57-4ccf-8f6a-876835937851",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_y.values)\n",
    "y_test = le.transform(test_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "execution_count": 12,
     "id": "302a5ad5-083d-444b-8dac-1a66c4e18262",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = 'wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {
     "execution_count": 13,
     "id": "6e10bb87-43a7-469b-8eda-364a6c1e0a88",
     "kernelId": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3427: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = load_fasttext(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyseCVxNVgor"
   },
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {
     "execution_count": 14,
     "id": "76841268-d48b-4682-aef0-0e889cbd4578",
     "kernelId": ""
    },
    "id": "JiufnWAmY3zY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "execution_count": 15,
     "id": "d10a7165-99c5-452f-aeaf-364863d34c71",
     "kernelId": ""
    },
    "id": "bCGewCsiVgor",
    "outputId": "9278ef04-b3df-4df9-b96e-80c4cac8e359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deive type:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = 40000\n",
    "seq_len = 500\n",
    "num_labels = 16\n",
    "EMBEDDING_DIM=embed_size\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print(\"deive type: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 16,
     "id": "07270060-b9f8-4efd-87cf-79e5da1c8012",
     "kernelId": ""
    },
    "id": "wtqLMdzkVgos"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_post, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_post)\n",
    "    # label must be in the same size as target\n",
    "    label_list = torch.tensor(label_list)\n",
    "    text_list = torch.stack(text_list)\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 17,
     "id": "7628cb49-f992-42c0-b90b-6470e4d8be9b",
     "kernelId": ""
    },
    "id": "hr5z3KUuVgos"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# # create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train).to(device), torch.tensor(y_train).to(device))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test).to(device),torch.tensor(y_test).to(device))\n",
    "# val_data = TensorDataset(torch.from_numpy(x_val).to(device),torch.tensor(y_val).to(device))\n",
    "\n",
    "# # dataloaders\n",
    "batch_size = 256\n",
    "\n",
    "# # make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "# val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "d8619495-e6d2-4ee7-a724-5739b0d0e173",
     "kernelId": ""
    },
    "id": "RBFUAADsVgos"
   },
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 18,
     "id": "03f97cd1-3cb4-4b6c-9662-64d257ad7ca2",
     "kernelId": ""
    },
    "id": "RkPRDGOIVgos"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 19,
     "id": "27113b65-5142-4ef8-bc8e-3da07e7ea65f",
     "kernelId": ""
    },
    "id": "XJ3NLN2DVgos"
   },
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.scalar = nn.Linear(self.input_dim,num_labels,bias=False)\n",
    "\n",
    "    def forward(self, M, x=None):\n",
    "        \"\"\"\n",
    "        M -> (seq_len, batch, vector)\n",
    "        x -> dummy argument for the compatibility with MatchingAttention\n",
    "        \"\"\"\n",
    "        scale = self.scalar(M) # seq_len, batch, 1\n",
    "        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n",
    "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n",
    "\n",
    "        return attn_pool, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 23,
     "id": "bd160baa-11ca-49aa-a1d4-86a689ce4d8a",
     "kernelId": ""
    },
    "id": "MhHqRM3VVgot"
   },
   "outputs": [],
   "source": [
    "class LSTMcustom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # embeddingbag outputs the average of all the words in a sentence\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "#         self.cnn = torch.nn.Conv1d(embed_size,20,2)\n",
    "        self.lstm = nn.LSTM(embed_size, 200, 1, bidirectional=False, batch_first = True)      \n",
    "        self.attention = SimpleAttention(200)  \n",
    "        self.linears = nn.Sequential(\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(100, num_labels),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "        \n",
    "                \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # (batch_size, sent_len, emb_size)\n",
    "\n",
    "        lstm_out,_ = self.lstm(embedded) \n",
    "        lstm_out = lstm_out.permute(1,0,2)\n",
    "        \n",
    "        atten_out, alpha = self.attention(lstm_out)       \n",
    "        \n",
    "#         return torch.log_softmax(torch.tanh(self.linears(atten_out)),1)\n",
    "        return torch.log_softmax(self.linears(atten_out),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 24,
     "id": "e0b54deb-4197-407a-842d-cd804cbdc9cb",
     "kernelId": ""
    },
    "id": "jkcx-phnVgot"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "#     model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 20\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        predicted_label = model(text)\n",
    "\n",
    "        # calculate loss and backpropagate to model paramters\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # update parameters by stepping the optimizer\n",
    "        optimizer.step()\n",
    "        predicted_label = torch.argmax(predicted_label,1)\n",
    "        total_acc += (predicted_label == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}| loss {:8f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count,loss.item()))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "#     print('val accuracy {:8.2f} | val loss {:8f}'.format(total_acc/total_count,loss.item()))\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 26,
     "id": "0ca1f9c8-0e57-4226-ba75-01934e0bbf0c",
     "kernelId": ""
    },
    "id": "d2vrnltsVgot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    20/   26 batches | accuracy    0.190| loss 2.362297\n",
      "| epoch   2 |    20/   26 batches | accuracy    0.205| loss 2.312289\n",
      "| epoch   3 |    20/   26 batches | accuracy    0.207| loss 2.345776\n",
      "| epoch   4 |    20/   26 batches | accuracy    0.291| loss 2.098326\n",
      "| epoch   5 |    20/   26 batches | accuracy    0.395| loss 1.872772\n",
      "| epoch   6 |    20/   26 batches | accuracy    0.509| loss 1.591138\n",
      "| epoch   7 |    20/   26 batches | accuracy    0.608| loss 1.349994\n",
      "| epoch   8 |    20/   26 batches | accuracy    0.635| loss 1.312217\n",
      "| epoch   9 |    20/   26 batches | accuracy    0.675| loss 1.217699\n",
      "| epoch  10 |    20/   26 batches | accuracy    0.706| loss 1.121703\n",
      "| epoch  11 |    20/   26 batches | accuracy    0.749| loss 0.909326\n",
      "| epoch  12 |    20/   26 batches | accuracy    0.775| loss 0.815796\n",
      "| epoch  13 |    20/   26 batches | accuracy    0.794| loss 0.763319\n",
      "| epoch  14 |    20/   26 batches | accuracy    0.828| loss 0.611910\n",
      "| epoch  15 |    20/   26 batches | accuracy    0.859| loss 0.568570\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 15 # epoch\n",
    "\n",
    "model = LSTMcustom().to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "total_accu = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "662fa6ff-7e86-4e3f-891a-c31a53f20826",
     "kernelId": ""
    },
    "id": "JXRqDqRYVgou"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy     0.63%\n"
     ]
    }
   ],
   "source": [
    "accu_test = evaluate(test_loader)\n",
    "print('test accuracy {:8.2f}%'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "56215bef-641c-4fed-87c2-3346ad338942",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
