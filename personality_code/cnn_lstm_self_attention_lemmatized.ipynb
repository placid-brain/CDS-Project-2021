{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0-ORnNrVgoe"
   },
   "source": [
    "## Balance data\n",
    "\n",
    "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzly-KMSVgoh"
   },
   "source": [
    "## SJ's Code\n",
    "\n",
    "https://github.com/declare-lab/conv-emotion/blob/0c9dcb9cc5234a7ca8cf6af81aabe28ef3814d0e/DialogueRNN/train_E2E.py#L81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM4cuug2Vgoh"
   },
   "source": [
    "### load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "whBeBw94Vgoi",
    "outputId": "e7387658-ffa8-4484-b57f-0f474bea9ef9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts_length</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>304</td>\n",
       "      <td>enfp intj moments sportscenter top ten play pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>554</td>\n",
       "      <td>find lack post alarm sex bore position often e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>427</td>\n",
       "      <td>good one course say know bless curse absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>521</td>\n",
       "      <td>dear intp enjoy conversation day esoteric gabb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>477</td>\n",
       "      <td>fire another silly misconception approach logi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  posts_length                                         lemmatized\n",
       "0  INFJ           304  enfp intj moments sportscenter top ten play pr...\n",
       "1  ENTP           554  find lack post alarm sex bore position often e...\n",
       "2  INTP           427  good one course say know bless curse absolutel...\n",
       "3  INTJ           521  dear intp enjoy conversation day esoteric gabb...\n",
       "4  ENTJ           477  fire another silly misconception approach logi..."
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('mbti_rm_stop_lemmatized.csv', index_col=None) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTERk6DAVgok"
   },
   "source": [
    "### encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "KOEKiuHHVgok"
   },
   "outputs": [],
   "source": [
    "int2mbti={0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISTJ',15:'ISTP'}\n",
    "mbti2int={'ENFJ':0,'ENFP':1,'ENTJ':2,'ENTP':3,'ESFJ':4,'ESFP':5,'ESTJ':6,'ESTP':7,'INFJ':8,'INFP':9,'INTJ':10,'INTP':11,'ISFJ':12,'ISFP':13,'ISTJ':14,'ISTP':15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jk4TsUBYVgok",
    "outputId": "df80d3ec-41f9-47e7-fb84-d86a3d6f3974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 10, 1, 15, 11, 13, 9, 14, 3, 9, 8, 9, 10, 9, 3, 11, 9, 8, 3, 8, 8, 9, 11, 9, 9, 8, 15, 12, 9, 14, 8, 8, 10, 4, 1, 9, 3, 10, 11, 11, 3, 10, 3, 10, 8, 13, 1, 11, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "labels = df.type.tolist()\n",
    "labels = [mbti2int.get(label) for label in labels]\n",
    "print(labels[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqwhztRQVgol"
   },
   "source": [
    "### clean posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "euKkHNpGXFWw",
    "outputId": "ed516ae6-6f5b-4ab9-addd-3630764abc83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /opt/conda/lib/python3.6/site-packages (2.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vjp0Z2JqVgol",
    "outputId": "4a812a7c-3038-429d-8735-25a713d067c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from autocorrect import Speller \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class data_preprocessing():\n",
    "    \n",
    "    def remove_links(text):\n",
    "        remove_https = re.sub(r'http\\S+', '', text)\n",
    "        remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "        return remove_com\n",
    "    \n",
    "    def remove_digits(text):\n",
    "        return re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    def remove_symbols(text):\n",
    "        REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "        BAD_SYMBOLS_RE = re.compile('(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
    "        t = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "        t = BAD_SYMBOLS_RE.sub(' ', t)\n",
    "        return t\n",
    "    \n",
    "    def deduce_repeated_characters(text):\n",
    "        Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "        Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "        Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "        Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "        Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "        return Final_Formatted\n",
    "    \n",
    "    def remove_special_characters(text):\n",
    "        return re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text)\n",
    "    \n",
    "    def spelling_correction(text):\n",
    "        spell = Speller(lang='en')\n",
    "        Corrected_text = spell(text)\n",
    "        return Corrected_text\n",
    "    \n",
    "    def lemmatization(text):\n",
    "        w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "id": "5vRxZq8gVgom"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = t.replace(\"|||\",\" \")   \n",
    "    t = data_preprocessing.remove_links(t)\n",
    "    t = data_preprocessing.remove_digits(t)\n",
    "    t = data_preprocessing.remove_symbols(t)\n",
    "    t = data_preprocessing.deduce_repeated_characters(t)\n",
    "    t = data_preprocessing.remove_special_characters(t)\n",
    "#         t = data_preprocessing.spelling_correction(t)\n",
    "    t = data_preprocessing.lemmatization(t)\n",
    "    t = ' '.join(word for word in t.split() if word not in STOPWORDS) \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNVRUmSOVgom",
    "outputId": "a63720cd-6f80-4b9e-9081-16458f2723d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enfp intj moments sportscenter top ten play prank life change experience life repeat today may perc experience immerse last thing infj friend post facebook commit suicide next day rest peace hello enfj sorry hear distress natural relationship perfection time every moment existence try figure hard time time growth welcome stuff game set match prozac wellbrutin least thirty minutes move legs mean move sit desk chair weed moderation maybe try edibles healthy alternative basically come three items determine type whichever type want would likely use give type cognitive function whatnot leave things moderation sims indeed video game good one note good one somewhat subjective completely promote death give sim dear enfp favorite video game grow current favorite video game cool appear late sad someone everyone wait think confidence good thing cherish time solitude revel within inner world whereas time workin enjoy time worry people always around yo entp ladies complimentary personality well hey main social outlet xbox live conversations even verbally fatigue quickly really dig part ban thread require get high backyard roast eat marshmellows backyard converse something intellectual follow massage kiss ban many sentence could think ban watch movies corner dunces ban health class clearly teach nothing peer pressure ban whole host reason two baby deer leave right munch beetle middle use blood two cavemen diary today late happen designate cave diary wall see pokemon world infj society everyone become optimist artists artists draw idea count form something like signature welcome robot rank person down self esteem cuz avid signature artist like proud ban take room bed ya get ta learn share roach ban much thunder grumble kind storm yep ahh old high school music hear age fail public speak class years ago sort learn could good position big part failure overload like person mentality confirm intj way move denver area start new life \n"
     ]
    }
   ],
   "source": [
    "posts = df.lemmatized.tolist()\n",
    "# posts = [clean_text(post) for post in posts]\n",
    "print(posts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwhmFmdTVgon"
   },
   "source": [
    "### create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "id": "4-F31ZAMVgon"
   },
   "outputs": [],
   "source": [
    "# Count total words\n",
    "from collections import Counter\n",
    "\n",
    "word_count=Counter()\n",
    "for post in posts:\n",
    "    if isinstance(post, str):\n",
    "        word_count.update(post.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JB7r2b4Vgoo",
    "outputId": "b6de5268-53b7-4200-b422-85a604138f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84822\n",
      "1946\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary available to the RNN\n",
    "vocab_len=len(word_count)\n",
    "print(vocab_len)\n",
    "\n",
    "print(len(posts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cag_e7yVgoo"
   },
   "source": [
    "### encode posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yruTbfJGVgop",
    "outputId": "27f909eb-1438-4663-9f8e-7dff644f6ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "# Create a look up table \n",
    "vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "# Create your dictionary that maps vocab words to integers here\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "posts_ints=[]\n",
    "for post in posts:\n",
    "    if isinstance(post, str):\n",
    "        posts_ints.append([vocab_to_int[word] for word in post.split()])\n",
    "\n",
    "# print(posts_ints[0])\n",
    "print(len(posts_ints[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsBMlGoIVgop",
    "outputId": "8807b7b4-e165-4774-e3c5-c7bb4b2efe5e"
   },
   "outputs": [],
   "source": [
    "import torchtext as text\n",
    "\n",
    "# load glove embeddings\n",
    "vec = text.vocab.GloVe(name='6B', dim=50)\n",
    "# create the embedding matrix, a torch tensor in the shape (num_words+1, embedding_dim)\n",
    "word_emb = vec.get_vecs_by_tokens(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzHMdaAoVgop"
   },
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5P7oxi5Vgoq",
    "outputId": "e082f2d1-b827-4ba6-8369-7abee59e9d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 6216\n",
      "Minimum review length: 33\n",
      "[[    0     0     0 ...    66   119    38]\n",
      " [   21   424    45 ...   602   377  1030]\n",
      " [    0     0     0 ...  1622  1140   200]\n",
      " ...\n",
      " [  147   563  2826 ...   119     1   432]\n",
      " [    0     0     0 ... 13468  6006    92]\n",
      " [  873    51   112 ...  1509  8225    33]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "posts_lens = Counter([len(x) for x in posts])\n",
    "print(\"Zero-length reviews: {}\".format(posts_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(posts_lens)))\n",
    "print(\"Minimum review length: {}\".format(min(posts_lens)))\n",
    "\n",
    "\n",
    "seq_len = 500\n",
    "features=np.zeros((len(posts_ints),seq_len),dtype=int)\n",
    "for i, row in enumerate(posts_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "print(features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pntpEmVYVgoq",
    "outputId": "c9ede30f-c0ad-40ed-cfd1-61ea6984b646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8674\n",
      "8674\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyseCVxNVgor"
   },
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "JiufnWAmY3zY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCGewCsiVgor",
    "outputId": "9278ef04-b3df-4df9-b96e-80c4cac8e359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deive type:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = 40000\n",
    "seq_len = 500\n",
    "num_labels = 16\n",
    "EMBEDDING_DIM=50\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "print(\"deive type: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJ7gVz9TVgos",
    "outputId": "4d20347f-06de-42b6-fc8c-aa518d224a24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6939 6939\n",
      "867 867\n",
      "868 868\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, random_state=50, \n",
    "                                                    test_size=0.2, stratify = labels )\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, random_state=0, \n",
    "                                                    test_size=0.5, stratify = y_test )\n",
    "\n",
    "print(len(x_train),len(y_train))\n",
    "print(len(x_test),len(y_test))\n",
    "print(len(x_val),len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "wtqLMdzkVgos"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_post, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_post)\n",
    "    # label must be in the same size as target\n",
    "    label_list = torch.tensor(label_list)\n",
    "    text_list = torch.stack(text_list)\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "hr5z3KUuVgos"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# # create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train).to(device), torch.tensor(y_train).to(device))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test).to(device),torch.tensor(y_test).to(device))\n",
    "val_data = TensorDataset(torch.from_numpy(x_val).to(device),torch.tensor(y_val).to(device))\n",
    "\n",
    "# # dataloaders\n",
    "batch_size = 256\n",
    "\n",
    "# # make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBFUAADsVgos"
   },
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "RkPRDGOIVgos"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "XJ3NLN2DVgos"
   },
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.scalar = nn.Linear(self.input_dim,num_labels,bias=False)\n",
    "\n",
    "    def forward(self, M, x=None):\n",
    "        \"\"\"\n",
    "        M -> (seq_len, batch, vector)\n",
    "        x -> dummy argument for the compatibility with MatchingAttention\n",
    "        \"\"\"\n",
    "        scale = self.scalar(M) # seq_len, batch, 1\n",
    "        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n",
    "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n",
    "\n",
    "        return attn_pool, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "MhHqRM3VVgot"
   },
   "outputs": [],
   "source": [
    "# logistic model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, word_vec, embed_dim):\n",
    "        super().__init__()\n",
    "        # embeddingbag outputs the average of all the words in a sentence\n",
    "        self.embedding = nn.Embedding(*(word_vec.size())).from_pretrained(word_vec, freeze=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_labels)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network parameters \n",
    "        \"\"\"\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # (batch_size, sent_len, emb_size)\n",
    "        embedded = embedded.sum(dim = 1) / lengths[:, None] # (add one axis)\n",
    "        return torch.sigmoid(self.fc(embedded))\n",
    "\n",
    "class LSTMcustom(nn.Module):\n",
    "    def __init__(self, word_vec, embed_dim):\n",
    "        super().__init__()\n",
    "        # embeddingbag outputs the average of all the words in a sentence\n",
    "        self.embedding = nn.Embedding(*(word_vec.size())).from_pretrained(word_vec, freeze=False)\n",
    "        self.cnn = torch.nn.Conv1d(embed_dim,20,2)\n",
    "        self.lstm = nn.LSTM(20, 200, 1, bidirectional=False, batch_first = True)      \n",
    "        self.attention = SimpleAttention(200)  \n",
    "        self.linears = nn.Sequential(\n",
    "            nn.Linear(200, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, num_labels),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "        \n",
    "                \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text) # (batch_size, sent_len, emb_size)\n",
    "        # print(\"embedded: \", embedded.size())\n",
    "        \n",
    "        embedded = embedded.permute(0,2,1)\n",
    "        cnn_out = self.cnn(embedded)\n",
    "        # print(\"cnn_out: \", cnn_out.size())\n",
    "        \n",
    "        cnn_out = cnn_out.permute(0,2,1)\n",
    "        lstm_out,_ = self.lstm(cnn_out) # lstm_out is a 3d tensor (batch_size, seq_len, output_size). If you have a bidirectional LSTM, the outputsize will be 2*output_size\n",
    "        # print(\"lstm_out: \", lstm_out.size())\n",
    "        lstm_out = lstm_out.permute(1,0,2)\n",
    "\n",
    "        atten_out, alpha = self.attention(lstm_out)       \n",
    "        # print(\"atten_out: \", atten_out.size()) \n",
    "        # print(\"atten_out (fc): \",self.fc(atten_out).size())\n",
    "        \n",
    "        return torch.log_softmax(torch.tanh(self.linears(atten_out)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "jkcx-phnVgot"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "#     model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 20\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text, label) in enumerate(dataloader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        predicted_label = model(text)\n",
    "        # label = torch.reshape(label,(len(label),1))\n",
    "        # calculate loss and backpropagate to model paramters\n",
    "#         print(\"predicted label size: \",predicted_label.size())\n",
    "#         print(\"label size: \",label.size())\n",
    "        loss = criterion(predicted_label, label)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        # update parameters by stepping the optimizer\n",
    "        optimizer.step()\n",
    "        predicted_label = torch.argmax(predicted_label,1)\n",
    "        # print(predicted_label)\n",
    "        total_acc += (predicted_label == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}| loss {:8f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count,loss.item()))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "#             evaluate(val_loader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    print('val accuracy {:8.2f} | val loss {:8f}'.format(total_acc/total_count,loss.item()))\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "id": "d2vrnltsVgot"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    20/   28 batches | accuracy    0.195| loss 2.336855\n",
      "| epoch   2 |    20/   28 batches | accuracy    0.211| loss 2.345699\n",
      "| epoch   3 |    20/   28 batches | accuracy    0.211| loss 2.311393\n",
      "| epoch   4 |    20/   28 batches | accuracy    0.214| loss 2.245397\n",
      "| epoch   5 |    20/   28 batches | accuracy    0.223| loss 2.073625\n",
      "| epoch   6 |    20/   28 batches | accuracy    0.231| loss 2.075429\n",
      "| epoch   7 |    20/   28 batches | accuracy    0.232| loss 2.077781\n",
      "| epoch   8 |    20/   28 batches | accuracy    0.240| loss 1.947394\n",
      "| epoch   9 |    20/   28 batches | accuracy    0.254| loss 1.938933\n",
      "| epoch  10 |    20/   28 batches | accuracy    0.246| loss 1.912148\n",
      "| epoch  11 |    20/   28 batches | accuracy    0.250| loss 1.856289\n",
      "| epoch  12 |    20/   28 batches | accuracy    0.275| loss 1.871767\n",
      "| epoch  13 |    20/   28 batches | accuracy    0.241| loss 1.935206\n",
      "| epoch  14 |    20/   28 batches | accuracy    0.270| loss 1.855201\n",
      "| epoch  15 |    20/   28 batches | accuracy    0.273| loss 1.913345\n",
      "| epoch  16 |    20/   28 batches | accuracy    0.282| loss 1.836306\n",
      "| epoch  17 |    20/   28 batches | accuracy    0.284| loss 1.937286\n",
      "| epoch  18 |    20/   28 batches | accuracy    0.282| loss 1.841645\n",
      "| epoch  19 |    20/   28 batches | accuracy    0.293| loss 1.901244\n",
      "| epoch  20 |    20/   28 batches | accuracy    0.280| loss 1.930929\n",
      "| epoch  21 |    20/   28 batches | accuracy    0.287| loss 1.944733\n",
      "| epoch  22 |    20/   28 batches | accuracy    0.307| loss 1.860075\n",
      "| epoch  23 |    20/   28 batches | accuracy    0.297| loss 1.858480\n",
      "| epoch  24 |    20/   28 batches | accuracy    0.319| loss 1.833362\n",
      "| epoch  25 |    20/   28 batches | accuracy    0.304| loss 1.815380\n",
      "| epoch  26 |    20/   28 batches | accuracy    0.314| loss 1.722702\n",
      "| epoch  27 |    20/   28 batches | accuracy    0.318| loss 1.740851\n",
      "| epoch  28 |    20/   28 batches | accuracy    0.335| loss 1.712085\n",
      "| epoch  29 |    20/   28 batches | accuracy    0.331| loss 1.805660\n",
      "| epoch  30 |    20/   28 batches | accuracy    0.336| loss 1.726384\n",
      "| epoch  31 |    20/   28 batches | accuracy    0.323| loss 1.828316\n",
      "| epoch  32 |    20/   28 batches | accuracy    0.332| loss 1.713170\n",
      "| epoch  33 |    20/   28 batches | accuracy    0.332| loss 1.714121\n",
      "| epoch  34 |    20/   28 batches | accuracy    0.348| loss 1.729741\n",
      "| epoch  35 |    20/   28 batches | accuracy    0.338| loss 1.689407\n",
      "| epoch  36 |    20/   28 batches | accuracy    0.351| loss 1.780099\n",
      "| epoch  37 |    20/   28 batches | accuracy    0.351| loss 1.786889\n",
      "| epoch  38 |    20/   28 batches | accuracy    0.370| loss 1.707770\n",
      "| epoch  39 |    20/   28 batches | accuracy    0.353| loss 1.700153\n",
      "| epoch  40 |    20/   28 batches | accuracy    0.356| loss 1.759678\n",
      "| epoch  41 |    20/   28 batches | accuracy    0.364| loss 1.700456\n",
      "| epoch  42 |    20/   28 batches | accuracy    0.359| loss 1.663049\n",
      "| epoch  43 |    20/   28 batches | accuracy    0.361| loss 1.760896\n",
      "| epoch  44 |    20/   28 batches | accuracy    0.356| loss 1.730439\n",
      "| epoch  45 |    20/   28 batches | accuracy    0.365| loss 1.710138\n",
      "| epoch  46 |    20/   28 batches | accuracy    0.356| loss 1.729925\n",
      "| epoch  47 |    20/   28 batches | accuracy    0.362| loss 1.684970\n",
      "| epoch  48 |    20/   28 batches | accuracy    0.372| loss 1.708984\n",
      "| epoch  49 |    20/   28 batches | accuracy    0.374| loss 1.690353\n",
      "| epoch  50 |    20/   28 batches | accuracy    0.371| loss 1.714772\n",
      "| epoch  51 |    20/   28 batches | accuracy    0.353| loss 1.687105\n",
      "| epoch  52 |    20/   28 batches | accuracy    0.369| loss 1.751022\n",
      "| epoch  53 |    20/   28 batches | accuracy    0.369| loss 1.635409\n",
      "| epoch  54 |    20/   28 batches | accuracy    0.369| loss 1.699753\n",
      "| epoch  55 |    20/   28 batches | accuracy    0.422| loss 1.673395\n",
      "| epoch  56 |    20/   28 batches | accuracy    0.436| loss 1.634359\n",
      "| epoch  57 |    20/   28 batches | accuracy    0.449| loss 1.628038\n",
      "| epoch  58 |    20/   28 batches | accuracy    0.433| loss 1.637360\n",
      "| epoch  59 |    20/   28 batches | accuracy    0.457| loss 1.571144\n",
      "| epoch  60 |    20/   28 batches | accuracy    0.456| loss 1.590364\n",
      "| epoch  61 |    20/   28 batches | accuracy    0.448| loss 1.615956\n",
      "| epoch  62 |    20/   28 batches | accuracy    0.448| loss 1.674989\n",
      "| epoch  63 |    20/   28 batches | accuracy    0.459| loss 1.635924\n",
      "| epoch  64 |    20/   28 batches | accuracy    0.455| loss 1.616690\n",
      "| epoch  65 |    20/   28 batches | accuracy    0.452| loss 1.634718\n",
      "| epoch  66 |    20/   28 batches | accuracy    0.458| loss 1.601244\n",
      "| epoch  67 |    20/   28 batches | accuracy    0.468| loss 1.583419\n",
      "| epoch  68 |    20/   28 batches | accuracy    0.466| loss 1.609676\n",
      "| epoch  69 |    20/   28 batches | accuracy    0.473| loss 1.597829\n",
      "| epoch  70 |    20/   28 batches | accuracy    0.461| loss 1.555400\n",
      "| epoch  71 |    20/   28 batches | accuracy    0.476| loss 1.533002\n",
      "| epoch  72 |    20/   28 batches | accuracy    0.459| loss 1.539863\n",
      "| epoch  73 |    20/   28 batches | accuracy    0.456| loss 1.534527\n",
      "| epoch  74 |    20/   28 batches | accuracy    0.467| loss 1.533464\n",
      "| epoch  75 |    20/   28 batches | accuracy    0.476| loss 1.472363\n",
      "| epoch  76 |    20/   28 batches | accuracy    0.462| loss 1.469676\n",
      "| epoch  77 |    20/   28 batches | accuracy    0.486| loss 1.547532\n",
      "| epoch  78 |    20/   28 batches | accuracy    0.475| loss 1.467653\n",
      "| epoch  79 |    20/   28 batches | accuracy    0.474| loss 1.503385\n",
      "| epoch  80 |    20/   28 batches | accuracy    0.488| loss 1.499868\n",
      "| epoch  81 |    20/   28 batches | accuracy    0.483| loss 1.417322\n",
      "| epoch  82 |    20/   28 batches | accuracy    0.475| loss 1.479631\n",
      "| epoch  83 |    20/   28 batches | accuracy    0.492| loss 1.458220\n",
      "| epoch  84 |    20/   28 batches | accuracy    0.483| loss 1.424961\n",
      "| epoch  85 |    20/   28 batches | accuracy    0.487| loss 1.500702\n",
      "| epoch  86 |    20/   28 batches | accuracy    0.498| loss 1.451856\n",
      "| epoch  87 |    20/   28 batches | accuracy    0.499| loss 1.517006\n",
      "| epoch  88 |    20/   28 batches | accuracy    0.492| loss 1.437379\n",
      "| epoch  89 |    20/   28 batches | accuracy    0.489| loss 1.441192\n",
      "| epoch  90 |    20/   28 batches | accuracy    0.487| loss 1.480701\n",
      "| epoch  91 |    20/   28 batches | accuracy    0.495| loss 1.485990\n",
      "| epoch  92 |    20/   28 batches | accuracy    0.495| loss 1.478483\n",
      "| epoch  93 |    20/   28 batches | accuracy    0.496| loss 1.436480\n",
      "| epoch  94 |    20/   28 batches | accuracy    0.505| loss 1.437146\n",
      "| epoch  95 |    20/   28 batches | accuracy    0.494| loss 1.494000\n",
      "| epoch  96 |    20/   28 batches | accuracy    0.496| loss 1.426222\n",
      "| epoch  97 |    20/   28 batches | accuracy    0.499| loss 1.409915\n",
      "| epoch  98 |    20/   28 batches | accuracy    0.494| loss 1.424845\n",
      "| epoch  99 |    20/   28 batches | accuracy    0.491| loss 1.442675\n",
      "| epoch 100 |    20/   28 batches | accuracy    0.500| loss 1.468809\n",
      "| epoch 101 |    20/   28 batches | accuracy    0.499| loss 1.418537\n",
      "| epoch 102 |    20/   28 batches | accuracy    0.499| loss 1.434219\n",
      "| epoch 103 |    20/   28 batches | accuracy    0.497| loss 1.399161\n",
      "| epoch 104 |    20/   28 batches | accuracy    0.495| loss 1.376643\n",
      "| epoch 105 |    20/   28 batches | accuracy    0.504| loss 1.389200\n",
      "| epoch 106 |    20/   28 batches | accuracy    0.504| loss 1.386897\n",
      "| epoch 107 |    20/   28 batches | accuracy    0.501| loss 1.439060\n",
      "| epoch 108 |    20/   28 batches | accuracy    0.510| loss 1.421185\n",
      "| epoch 109 |    20/   28 batches | accuracy    0.504| loss 1.388230\n",
      "| epoch 110 |    20/   28 batches | accuracy    0.522| loss 1.401994\n",
      "| epoch 111 |    20/   28 batches | accuracy    0.516| loss 1.361448\n",
      "| epoch 112 |    20/   28 batches | accuracy    0.516| loss 1.386723\n",
      "| epoch 113 |    20/   28 batches | accuracy    0.545| loss 1.432017\n",
      "| epoch 114 |    20/   28 batches | accuracy    0.573| loss 1.371631\n",
      "| epoch 115 |    20/   28 batches | accuracy    0.592| loss 1.451721\n",
      "| epoch 116 |    20/   28 batches | accuracy    0.591| loss 1.437599\n",
      "| epoch 117 |    20/   28 batches | accuracy    0.645| loss 1.396588\n",
      "| epoch 118 |    20/   28 batches | accuracy    0.655| loss 1.367028\n",
      "| epoch 119 |    20/   28 batches | accuracy    0.665| loss 1.347988\n",
      "| epoch 120 |    20/   28 batches | accuracy    0.670| loss 1.361584\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 120 # epoch\n",
    "\n",
    "model = LSTMcustom(word_vec=word_emb, embed_dim=EMBEDDING_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "total_accu = None\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "JXRqDqRYVgou"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val accuracy     0.32 | val loss 2.176192\n",
      "test accuracy     0.32%\n"
     ]
    }
   ],
   "source": [
    "accu_test = evaluate(test_loader)\n",
    "print('test accuracy {:8.2f}%'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
